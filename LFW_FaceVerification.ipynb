{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2200 images in the train, 1000 in the test\n"
     ]
    }
   ],
   "source": [
    "#Divede the train, validation and test dataset.\n",
    "file_handler = open(\"../pairsDevTrain.txt\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# We must first put the data in a list\n",
    "train_same = [];\n",
    "train_diff= [];\n",
    "\n",
    "for row in file_handler:\n",
    "    # The values we read are strings, we must convert them to the correct type (float)\n",
    "    temp=row.replace(\"\\n\", \"\");\n",
    "    string= temp.split(\"\\t\") # split it by whitespace\n",
    "    converted = [x for x in string]\n",
    "    if len(converted) ==3:\n",
    "        #train_same.append( [converted[0],converted[1],converted[0],converted[2] ]);\n",
    "        train_same.append( converted );\n",
    "    if len(converted) ==4:\n",
    "        train_diff.append(converted);\n",
    "file_handler.close()\n",
    "\n",
    "\n",
    "file_handler = open(\"../pairsDevTest.txt\", \"r\", encoding=\"utf-8\")\n",
    "# We must first put the data in a list\n",
    "test_same = [];\n",
    "test_diff= [];\n",
    "\n",
    "for row in file_handler:\n",
    "    temp=row.replace(\"\\n\", \"\");\n",
    "    string= temp.split(\"\\t\") # split it by whitespace\n",
    "    converted = [x for x in string]\n",
    "    if len(converted) ==3:\n",
    "        test_same.append( converted );\n",
    "    if len(converted) ==4:\n",
    "        test_diff.append(converted);\n",
    "file_handler.close()\n",
    "\n",
    "print(\"There are {} images in the train, {} in the test\".format(len(train_same)+ len(train_diff), len(test_same)+ len(test_diff) ) );\n",
    "\n",
    "\n",
    "vali_same= train_same[0: 250: 1];\n",
    "vali_diff= train_diff[0: 250: 1];\n",
    "\n",
    "train_same=train_same[250: : 1]\n",
    "train_diff=train_diff[250: : 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 850, 250, 250, 500, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( train_same), len(train_diff), len(vali_same), len(vali_diff), len(test_same), len(test_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height, channels=96, 96, 3\n",
    "def format_filename(name, number):\n",
    "    num_zeros = \"0\"*(4 - len(number))\n",
    "    filepath = \"../lfw-deepfunneled/\"+name+\"/\"+name+\"_\"+num_zeros+number+\".jpg\"\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def extractFaceCo (imagePath): \n",
    "    cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "    # Create the haar cascade\n",
    "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(imagePath ) #read a RGB image\n",
    "    #print(image.shape)\n",
    "    # Detect faces in the image\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        image,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    #flags = cv2.CV_HAAR_SCALE_IMAGE\n",
    "    )\n",
    "     \n",
    "    # Draw a rectangle around the faces\n",
    "    if len(faces)==0:  #no face is detected\n",
    "        print(imagePath)\n",
    "        return np.array([])\n",
    "    \n",
    "    best_x, best_y, best_w, best_h=0,0,250,250;\n",
    "    best_size= -np.float(\"inf\")\n",
    "    for (x, y, w, h) in faces: #There might be more than one face\n",
    "        if w*h> best_size:\n",
    "            best_size= w*h\n",
    "            best_x, best_y, best_w, best_h= x, y, w, h\n",
    "    \n",
    "    \n",
    "    image1= image[ best_y:best_y+best_h, best_x:best_x+best_w ]\n",
    "    face= cv2.resize(image1, (width, height), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    #print(face.shape)\n",
    "    #print(face[0, 0:5])\n",
    "    return face #is returning a local variable legal? \n",
    "\n",
    "\n",
    "def get_batch(batch_size, same, diff):\n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "\n",
    "    pairs=[np.zeros( (batch_size, height, width, channels) ) for i in range(2)]    \n",
    "    # initialize vector for the targets\n",
    "    targets=np.zeros( batch_size )\n",
    "    \n",
    "    num_fails=0;\n",
    "    not_enough=0\n",
    "    for i in range( batch_size//2 ):\n",
    "        \n",
    "        img1= extractFaceCo(format_filename( same[i][0], same[i][1] )  )\n",
    "        img2= extractFaceCo(format_filename( same[i][0], same[i][2] )  )        \n",
    "        while (len(img1)==0 or len(img2)==0): #fails to detect the face\n",
    "            num_fails+=1\n",
    "            if (i+ num_fails >= len(same) ):\n",
    "                print(\"Did not find enough qualified images, please reduce the batch_size\")\n",
    "                not_enough=1;\n",
    "                break;\n",
    "            \n",
    "            img1= extractFaceCo(format_filename( same[-num_fails][0], same[-num_fails][1] )  )\n",
    "            img2= extractFaceCo(format_filename( same[-num_fails][0], same[-num_fails][2] )  )\n",
    "        \n",
    "        if not_enough:\n",
    "            break;\n",
    "        pairs[0][i, :, :, :]= img1.reshape(height, width, -1)\n",
    "        pairs[1][i, :, :, :]= img2.reshape(height, width, -1)   \n",
    "        targets[i]= 1;\n",
    "        \n",
    "        #print to check whether it is correct\n",
    "        #print(\"now in get_batch\")\n",
    "        #print(img1[0, 0:5])\n",
    "        #print( pairs[0][i, 0, 0:5, 0])\n",
    "        \n",
    "    num_fails-= num_fails;  \n",
    "    not_enough=0\n",
    "    for i in range( batch_size//2, batch_size ):\n",
    "        j= i- batch_size//2; #j start from 0\n",
    "        img1= extractFaceCo(format_filename( diff[j][0], diff[j][1] )  )\n",
    "        img2= extractFaceCo(format_filename( diff[j][2], diff[j][3] )  )\n",
    "            \n",
    "        while (len(img1)==0 or len(img2)==0): #fails to detect the face\n",
    "            num_fails+=1\n",
    "            if (j+ num_fails >= len(diff) ): #i+num_fails== len(diff): they are pointing to the same element.\n",
    "                print(\"Did not find enough qualified images, please reduce the batch_size\")\n",
    "                not_enough=1;\n",
    "                break;\n",
    "            img1= extractFaceCo(format_filename( diff[-num_fails][0], diff[-num_fails][1] )  )\n",
    "            img2= extractFaceCo(format_filename( diff[-num_fails][2], diff[-num_fails][3] )  )\n",
    "        \n",
    "        if not_enough:\n",
    "            break;    \n",
    "        pairs[0][i, :, :, :]= img1\n",
    "        pairs[1][i, :, :, :]= img2\n",
    "        targets[i]= 0;  \n",
    "        \n",
    "        #print to check whether it is correct\n",
    "        #print(\"now in get_batch\")\n",
    "        #print(img2[0, 0:5])\n",
    "        #print( pairs[1][i, 0, 0:5, 0])\n",
    "    return pairs, targets\n",
    "\n",
    "def fillIdent(batch_size, same, diff):       \n",
    "    name={}\n",
    "    ident_in=[];\n",
    "    indet_out=[];\n",
    "    ident_y=[]\n",
    "    \n",
    "    i=0    \n",
    "    num_fails=0;\n",
    "    num_same=0;\n",
    "    not_enough=0\n",
    "    while num_same< batch_size//2 and i< len(same):\n",
    "        if same[i][0] in name:\n",
    "            i+=1\n",
    "            continue\n",
    "        name[ same[i][0] ]=1;   \n",
    "        img1= extractFaceCo(format_filename( same[i][0], same[i][1] )  )\n",
    "        img2= extractFaceCo(format_filename( same[i][0], same[i][2] )  )        \n",
    "        while (len(img1)==0 or len(img2)==0): #fails to detect the face\n",
    "            num_fails+=1\n",
    "            if (i+ num_fails >= len(same) ):\n",
    "                print(\"Did not find enough qualified images, please reduce the batch_size\")\n",
    "                not_enough=1\n",
    "                break;\n",
    "            \n",
    "            img1= extractFaceCo(format_filename( same[-num_fails][0], same[-num_fails][1] )  )\n",
    "            img2= extractFaceCo(format_filename( same[-num_fails][0], same[-num_fails][2] )  )\n",
    "            \n",
    "        if not_enough:\n",
    "            break;\n",
    "        num_same+=1    \n",
    "        ident_in.append( img1.reshape(height, width, -1) )\n",
    "        ident_out.append( img2.reshape(height, width, -1)  )\n",
    "        ident_y.append(1);\n",
    "        \n",
    "    i=0    \n",
    "    num_fails=0;\n",
    "    num_diff=0\n",
    "    not_enough=0\n",
    "    while num_same+ num_diff< batch_size and i< len(diff):\n",
    "        if diff[i][0] in name:\n",
    "            i+=1\n",
    "            continue\n",
    "        name[ diff[i][0] ]=1\n",
    "            \n",
    "        img1= extractFaceCo(format_filename( diff[i][0], diff[i][1] )  )\n",
    "        img2= extractFaceCo(format_filename( diff[i][2], diff[i][3] )  )      \n",
    "        while (len(img1)==0 or len(img2)==0): #fails to detect the face\n",
    "            num_fails+=1\n",
    "            if (i+ num_fails >= len(same) ):\n",
    "                print(\"Did not find enough qualified images, please reduce the batch_size\")\n",
    "                not_enough=1;\n",
    "                break;\n",
    "            \n",
    "            img1= extractFaceCo(format_filename( diff[-num_fails][0], diff[-num_fails][1] )  )\n",
    "            img2= extractFaceCo(format_filename( diff[-num_fails][2], diff[-num_fails][3] )  )\n",
    "            \n",
    "        if not_enough:\n",
    "            break;\n",
    "        num_diff+=1;\n",
    "        ident_in.append( img1.reshape(height, width, -1) )\n",
    "        ident_out.append( img2.reshape(height, width, -1)  )\n",
    "        ident_y.append(0);\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../lfw-deepfunneled/George_P_Bush/George_P_Bush_0001.jpg\n",
      "../lfw-deepfunneled/Juan_Pablo_Montoya/Juan_Pablo_Montoya_0005.jpg\n",
      "../lfw-deepfunneled/Kate_Capshaw/Kate_Capshaw_0002.jpg\n",
      "../lfw-deepfunneled/Ken_Watanabe/Ken_Watanabe_0001.jpg\n",
      "../lfw-deepfunneled/Kobe_Bryant/Kobe_Bryant_0001.jpg\n",
      "../lfw-deepfunneled/Budd_Schulberg/Budd_Schulberg_0001.jpg\n",
      "../lfw-deepfunneled/Keith_Brown/Keith_Brown_0001.jpg\n",
      "../lfw-deepfunneled/Clive_Lloyd/Clive_Lloyd_0001.jpg\n",
      "../lfw-deepfunneled/Roy_Halladay/Roy_Halladay_0001.jpg\n",
      "../lfw-deepfunneled/Hernan_Diaz/Hernan_Diaz_0001.jpg\n",
      "../lfw-deepfunneled/Dereck_Whittenburg/Dereck_Whittenburg_0001.jpg\n",
      "../lfw-deepfunneled/Julio_Rossi/Julio_Rossi_0001.jpg\n",
      "../lfw-deepfunneled/Mickey_Rooney/Mickey_Rooney_0001.jpg\n",
      "../lfw-deepfunneled/Irwan_Fadzi_Idris/Irwan_Fadzi_Idris_0001.jpg\n",
      "../lfw-deepfunneled/Irwan_Fadzi_Idris/Irwan_Fadzi_Idris_0001.jpg\n",
      "../lfw-deepfunneled/Pele/Pele_0001.jpg\n",
      "../lfw-deepfunneled/Shingo_Katayama/Shingo_Katayama_0001.jpg\n",
      "start to work on validation\n",
      "start to work on test\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y= get_batch(1000, train_same, train_diff);\n",
    "print(\"start to work on validation\")\n",
    "vali_X, vali_y= get_batch(200, vali_same, vali_diff);\n",
    "print(\"start to work on test\")\n",
    "test_X, test_y= get_batch(20, test_same, test_diff);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: triplet_loss\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines)\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = K.sum( K.square( anchor- positive ), axis= -1 )\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = K.sum( K.square( anchor- negative ), axis= -1 )\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss =  pos_dist- neg_dist+ alpha\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = K.sum( K.maximum( basic_loss, 0.0 ) )\n",
    "    ### END CODE HERE ###    \n",
    "    return loss\n",
    "\n",
    "FRmodel = faceRecoModel( [3, 96, 96] )\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use abs of different\n",
    "EPSILON=0.0001\n",
    "class FaceVerification:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.encoding=[]\n",
    "        self.same_person=[]\n",
    "        self.score=0\n",
    "        self.encoding_vali=[]\n",
    "        \n",
    "        self.data_base= {}\n",
    "        \n",
    "        \n",
    "    def encodingImages(self, X, y): #list of 2 4 dimension array [batch, h, w, channels]\n",
    "        self.encoding.clear()\n",
    "        self.same_person.clear()\n",
    "        for i in range( len( y ) ): #X[0] and X[1] form image pairs\n",
    "            img1= X[0][i, :, :, :]  #(height, width, channel)\n",
    "            img2= X[1][i, :, :, :]  #(height, width, channel)\n",
    "            #encoding1= img_to_encoding( img1, FRmodel ).ravel()\n",
    "            #encoding2= img_to_encoding( img2, FRmodel).ravel()\n",
    "            #diff= (encoding1- encoding2)**2/ max(encoding1+ encoding2, EPSILON)\n",
    "            \n",
    "            \n",
    "            diff= np.abs( img_to_encoding( img1, FRmodel ).ravel()- img_to_encoding( img2, FRmodel).ravel() )\n",
    "            self.encoding.append( diff )  #list of 1d array\n",
    "            self.same_person.append( y[i] )  #list of numbers \n",
    "            \n",
    "    def encodingVali(self, X):\n",
    "        self.encoding_vali.clear()\n",
    "        for i in range( len( X[0] ) ): #X[0] and X[1] form image pairs\n",
    "            img1= X[0][i, :, :, :]  #(height, width, channel)\n",
    "            img2= X[1][i, :, :, :]  #(height, width, channel)\n",
    "            #encoding1= img_to_encoding( img1, FRmodel ).ravel()\n",
    "            #encoding2= img_to_encoding( img2, FRmodel).ravel()\n",
    "            #diff= (encoding1- encoding2)**2/ max(encoding1+ encoding2, EPSILON) #Chi^2 distance\n",
    "            diff= np.abs( img_to_encoding( img1, FRmodel  ).ravel()- img_to_encoding( img2, FRmodel  ).ravel() )\n",
    "            self.encoding_vali.append( diff )  #list of 1d array\n",
    "        \n",
    "    \n",
    "    def trainModelPredict(self, C): #Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "        model= LogisticRegression(penalty=\"l2\", dual=False, tol=0.0001, solver='lbfgs', C=C )\n",
    "        scores=cross_val_score(model, self.encoding, self.same_person, scoring=\"accuracy\", cv = 5)\n",
    "        self.score= scores.mean();\n",
    "        \n",
    "        model.fit(self.encoding, self.same_person)\n",
    "        \n",
    "        predicts=model.predict( self.encoding_vali)\n",
    "        return predicts, self.score\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Chi^2 distance (x-y)^2/(x+y)\n",
    "EPSILON=0.0001\n",
    "class FaceVerification:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.encoding=[]\n",
    "        self.same_person=[]\n",
    "        self.score=0\n",
    "        self.encoding_vali=[]\n",
    "        \n",
    "        self.data_base= {}\n",
    "        \n",
    "        \n",
    "    def encodingImages(self, X, y): #list of 2 4 dimension array [batch, h, w, channels]\n",
    "        self.encoding.clear()\n",
    "        self.same_person.clear()\n",
    "        for i in range( len( y ) ): #X[0] and X[1] form image pairs\n",
    "            img1= X[0][i, :, :, :]  #(height, width, channel)\n",
    "            img2= X[1][i, :, :, :]  #(height, width, channel)\n",
    "            #encoding1= img_to_encoding( img1, FRmodel ).ravel()\n",
    "            #encoding2= img_to_encoding( img2, FRmodel).ravel()\n",
    "            #diff= (encoding1- encoding2)**2/ max(encoding1+ encoding2, EPSILON)\n",
    "            \n",
    "            \n",
    "            diff= np.abs( img_to_encoding( img1, FRmodel ).ravel()- img_to_encoding( img2, FRmodel).ravel() )\n",
    "            self.encoding.append( diff )  #list of 1d array\n",
    "            self.same_person.append( y[i] )  #list of numbers \n",
    "            \n",
    "    def encodingVali(self, X):\n",
    "        self.encoding_vali.clear()\n",
    "        for i in range( len( X[0] ) ): #X[0] and X[1] form image pairs\n",
    "            img1= X[0][i, :, :, :]  #(height, width, channel)\n",
    "            img2= X[1][i, :, :, :]  #(height, width, channel)\n",
    "            #encoding1= img_to_encoding( img1, FRmodel ).ravel()\n",
    "            #encoding2= img_to_encoding( img2, FRmodel).ravel()\n",
    "            #diff= (encoding1- encoding2)**2/ max(encoding1+ encoding2, EPSILON) #Chi^2 distance\n",
    "            diff= np.abs( img_to_encoding( img1, FRmodel  ).ravel()- img_to_encoding( img2, FRmodel  ).ravel() )\n",
    "            self.encoding_vali.append( diff )  #list of 1d array\n",
    "        \n",
    "    \n",
    "    def trainModelPredict(self, C): #Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "        model= LogisticRegression(penalty=\"l2\", dual=False, tol=0.0001, solver='lbfgs', C=C )\n",
    "        scores=cross_val_score(model, self.encoding, self.same_person, scoring=\"accuracy\", cv = 5)\n",
    "        self.score= scores.mean();\n",
    "        \n",
    "        model.fit(self.encoding, self.same_person)\n",
    "        \n",
    "        predicts=model.predict( self.encoding_vali)\n",
    "        return predicts, self.score\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_veri=FaceVerification();\n",
    "face_veri.encodingImages( train_X, train_y )\n",
    "face_veri.encodingVali(vali_X);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[ ]\n",
    "for C in [0.01, 0.03, 0.1, 0.3,  1, 3, 10]:\n",
    "    predicts, score= face_veri.trainModelPredict(C)  \n",
    "    scores.append(score)\n",
    "predictions, score= face_veri.trainModelPredict(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, score= face_veri.trainModelPredict(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "TN=sum( (vali_y== predictions)& (vali_y==0)  )/sum((vali_y==0))\n",
    "TP=sum( (vali_y== predictions)& (vali_y==1)  )/sum((vali_y==1))\n",
    "print(TP, TN, (TP+TN)/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.a=3;\n",
    "        self.b=5\n",
    "        print(12)\n",
    "    def aa(self):\n",
    "        self.a=10\n",
    "        self.b=[1,2,3] #this is only a local variable. use = carefully: means to declare a new variable.\n",
    "    def cc(self):\n",
    "        print(self.b)\n",
    "        print(self.a)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[1, 2, 3]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "e=test()\n",
    "e.aa()\n",
    "e.cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=LogisticRegression(solver='lbfgs')\n",
    "b.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 3)\n",
      "(96, 96, 3)\n",
      "(3, 96, 96)\n",
      "(1, 128)\n"
     ]
    }
   ],
   "source": [
    "height, width=96, 96\n",
    "input_= np.zeros( (1, 3, height, width) )\n",
    "img= cv2.imread(\"database_images/Alexander_Lukashenko_0001.jpg\")\n",
    "print(img.shape)\n",
    "img2=cv2.resize(img, (96, 96), interpolation = cv2.INTER_AREA)\n",
    "print(img2.shape)\n",
    "img3=np.rollaxis(img2, 2, 0 )\n",
    "print(img3.shape)\n",
    "input_[0, : , : , : ]= img3\n",
    "a=img_to_encoding( img, FRmodel)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ 0.04595553  0.07246497  0.07501942  0.0937435   0.03919019  0.25245452\n",
      "  0.05546704 -0.06767073  0.03881087 -0.06296956  0.02604692  0.07036097\n",
      "  0.0349875  -0.10865398  0.06629271 -0.11094904 -0.0074172   0.09232423\n",
      " -0.09684844  0.01670366  0.01164339  0.03277656  0.02620441  0.21404396\n",
      "  0.02198745 -0.18915516 -0.09475376 -0.09374249 -0.06817286  0.02285943\n",
      "  0.13253497  0.03000964 -0.07266176  0.15166478  0.01422779  0.01063436\n",
      "  0.07596198 -0.05064294 -0.02171043  0.06110191  0.1504443  -0.06104789\n",
      " -0.01450685 -0.11921036 -0.13727635 -0.2175961   0.02953147  0.00640469\n",
      " -0.00888163  0.11729448  0.08539972 -0.14602092  0.01938699  0.02699455\n",
      " -0.01152262  0.0814869  -0.03831456  0.08872418 -0.01724762 -0.02097518\n",
      " -0.12677157  0.09929045  0.02552805 -0.3351959   0.08579529  0.01957398\n",
      "  0.1190557  -0.13662153 -0.26028576  0.08095471 -0.03529876  0.08136974\n",
      " -0.03445316  0.10672768  0.02060395 -0.0016467   0.03546085 -0.0476696\n",
      "  0.01591405  0.01360815 -0.07788026  0.03363461  0.06699368 -0.00204315\n",
      "  0.0119293   0.03546859  0.03746663 -0.07266721 -0.09266681  0.04582385\n",
      "  0.05123822 -0.17604174  0.11766151 -0.00583994 -0.03784202 -0.02725513\n",
      " -0.08658117 -0.01110868 -0.01644513  0.0929643   0.00063533  0.00158254\n",
      " -0.08582316  0.14856128  0.04525093  0.03654547  0.01645859 -0.06793622\n",
      " -0.16557387  0.070774   -0.06059191  0.05601184  0.09458261  0.01802875\n",
      " -0.06338678 -0.02514806 -0.02924109  0.05861189 -0.02785514  0.11836785\n",
      "  0.08724955 -0.04570846 -0.07120414  0.02354563  0.03395399  0.06259214\n",
      " -0.06447565 -0.03994617]\n",
      "[ 0.04595553  0.07246497  0.07501942  0.0937435   0.03919019  0.25245452\n",
      "  0.05546704 -0.06767073  0.03881087 -0.06296956  0.02604692  0.07036097\n",
      "  0.0349875  -0.10865398  0.06629271 -0.11094904 -0.0074172   0.09232423\n",
      " -0.09684844  0.01670366  0.01164339  0.03277656  0.02620441  0.21404396\n",
      "  0.02198745 -0.18915516 -0.09475376 -0.09374249 -0.06817286  0.02285943\n",
      "  0.13253497  0.03000964 -0.07266176  0.15166478  0.01422779  0.01063436\n",
      "  0.07596198 -0.05064294 -0.02171043  0.06110191  0.1504443  -0.06104789\n",
      " -0.01450685 -0.11921036 -0.13727635 -0.2175961   0.02953147  0.00640469\n",
      " -0.00888163  0.11729448  0.08539972 -0.14602092  0.01938699  0.02699455\n",
      " -0.01152262  0.0814869  -0.03831456  0.08872418 -0.01724762 -0.02097518\n",
      " -0.12677157  0.09929045  0.02552805 -0.3351959   0.08579529  0.01957398\n",
      "  0.1190557  -0.13662153 -0.26028576  0.08095471 -0.03529876  0.08136974\n",
      " -0.03445316  0.10672768  0.02060395 -0.0016467   0.03546085 -0.0476696\n",
      "  0.01591405  0.01360815 -0.07788026  0.03363461  0.06699368 -0.00204315\n",
      "  0.0119293   0.03546859  0.03746663 -0.07266721 -0.09266681  0.04582385\n",
      "  0.05123822 -0.17604174  0.11766151 -0.00583994 -0.03784202 -0.02725513\n",
      " -0.08658117 -0.01110868 -0.01644513  0.0929643   0.00063533  0.00158254\n",
      " -0.08582316  0.14856128  0.04525093  0.03654547  0.01645859 -0.06793622\n",
      " -0.16557387  0.070774   -0.06059191  0.05601184  0.09458261  0.01802875\n",
      " -0.06338678 -0.02514806 -0.02924109  0.05861189 -0.02785514  0.11836785\n",
      "  0.08724955 -0.04570846 -0.07120414  0.02354563  0.03395399  0.06259214\n",
      " -0.06447565 -0.03994617]\n"
     ]
    }
   ],
   "source": [
    "c=img_to_encoding( cv2.imread(\"database_images/Alexander_Lukashenko_0001.jpg\"), FRmodel).ravel()\n",
    "print( type(c) )\n",
    "print(c)\n",
    "print(c.ravel() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape\n",
    "b=[]\n",
    "b.append( a.ravel() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 3)\n",
      "(96, 96, 3)\n",
      "(96, 96, 3)\n",
      "(3, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "image= cv2.imread(\"database_images/Alexander_Lukashenko_0001.jpg\")\n",
    "print( image.shape )\n",
    "image = cv2.resize(image, (96, 96)) \n",
    "print( image.shape )\n",
    "img = image[...,::-1]\n",
    "print( img.shape )\n",
    "img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "print( img.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 3)\n",
      "(110, 110, 3)\n",
      "(3, 110, 110)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (3, 96, 96) but got array with shape (110, 96, 96)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0ca192be447e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimg3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/LFW/python/fr_utils.py\u001b[0m in \u001b[0;36mimg_to_encoding\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \"\"\"\n\u001b[0;32m-> 1268\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (3, 96, 96) but got array with shape (110, 96, 96)"
     ]
    }
   ],
   "source": [
    "img= cv2.imread(\"database_images/Alexander_Lukashenko_0001.jpg\")\n",
    "print(img.shape)\n",
    "img2=cv2.resize(img, (110, 110), interpolation = cv2.INTER_AREA)\n",
    "print(img2.shape)\n",
    "img3=np.rollaxis(img2, 2, 0 )\n",
    "print(img3.shape)\n",
    "a=img_to_encoding( img3, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[ [i, i] if i< 50 else [-i, i-50]  for i in range(100)]\n",
    "y=[ 1 if i< 50 else 0 for i in range(100)]\n",
    "X_vali=[[1, 2], [-60, 0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= FaceVerification()\n",
    "a.trainModel(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
